# export KAFKA_DATA=/Users/admin/docker_data/kafka
# docker-compose -f ./docker-compose-local-kafka-cluster.yml up -d
# ./start-kafka-shell.sh 192.168.1.7 192.168.1.7:2181
# docker-compose -f ./docker-compose-local-kafka-cluster.yml down

# $KAFKA_HOME/bin/kafka-topics.sh --create --topic topic --partitions 4 --zookeeper $ZK --replication-factor 2
# $KAFKA_HOME/bin/kafka-topics.sh --describe --topic topic --zookeeper $ZK
# echo "test"|$KAFKA_HOME/bin/kafka-console-producer.sh --topic=news_feed --broker-list=`broker-list.sh`
# $KAFKA_HOME/bin/kafka-console-consumer.sh --topic=topic --bootstrap-server 192.168.1.12:9095
# KAFKACAT : docker run -it --network=host edenhill/kcat:1.6.0 -b 192.168.1.12:9095 -L
# kcat -b 192.168.18.23:9092 -t "news_feed"

# RUNNING
# HOST_IP=192.168.18.23 docker-compose up -d

# STOPPING
# docker-compose down

version: '2'

services:
  redis:
    container_name: redis
    image: "redis"
    ports:
      - "6379:6379"
    command: redis-server --requirepass '42a6A#8epiwj?A:R?DqCKpn5f'
  mongo:
    image: mongo
    ports:
      - "27017:27017"
    restart: always
    environment:
      MONGO_INITDB_ROOT_USERNAME: root
      MONGO_INITDB_ROOT_PASSWORD: passw0rd
    # volumes: 
    #   - /Users/admin/docker_data/mongo:/data/db
  mongo-express:
    image: mongo-express
    restart: always
    ports:
      - 8081:8081
    environment:
      ME_CONFIG_MONGODB_ADMINUSERNAME: root
      ME_CONFIG_MONGODB_ADMINPASSWORD: passw0rd
      ME_CONFIG_MONGODB_URL: mongodb://root:passw0rd@mongo:27017/

  # UNCOMMENT THIS IF YOU WANT TO RUN BOTH ZOOKEEPER AND KAFKA AS CONTAINERS. COMMENTED THIS BECAUSE TOO MANY DOCKER CONTAINERS ON MY LAPTOP.
  # YOU CAN ALSO RUN ZK AND KAFKA AS A SERVICE (RECOMMENDED) INSTEAD OF A CONTAINER ON LAPTOP

  # zookeeper:
  #   image: confluentinc/cp-zookeeper:latest
  #   ports:
  #     - "2181:2181"
  #   environment:
  #     ZOOKEEPER_CLIENT_PORT: 2181
  #     ZOOKEEPER_TICK_TIME: 2000
  #     ZOO_MY_ID: 1

  # kafka:
  #   # "`-._,-'"`-._,-'"`-._,-'"`-._,-'"`-._,-'"`-._,-'"`-._,-'"`-._,-'"`-._,-
  #   # An important note about accessing Kafka from clients on other machines: 
  #   # -----------------------------------------------------------------------
  #   #
  #   # The config used here exposes port 29092 for _external_ connections to the broker
  #   # i.e. those from _outside_ the docker network. This could be from the host machine
  #   # running docker, or maybe further afield if you've got a more complicated setup. 
  #   # If the latter is true, you will need to change the value 'localhost' in 
  #   # KAFKA_ADVERTISED_LISTENERS to one that is resolvable to the docker host from those 
  #   # remote clients
  #   #
  #   # For connections _internal_ to the docker network, such as from other services
  #   # and components, use kafka:9092.
  #   #
  #   # See https://rmoff.net/2018/08/02/kafka-listeners-explained/ for details
  #   # "`-._,-'"`-._,-'"`-._,-'"`-._,-'"`-._,-'"`-._,-'"`-._,-'"`-._,-'"`-._,-
  #   #
  #   image: confluentinc/cp-kafka:latest
  #   depends_on:
  #     - zookeeper
  #   ports:
  #     - 9092:9092
  #     - 29092:29092
  #   environment:
  #     KAFKA_BROKER_ID: 1
  #     KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
  #     KAFKA_LISTENERS: INTERNAL://0.0.0.0:29092,EXTERNAL://0.0.0.0:9092
  #     KAFKA_ADVERTISED_LISTENERS: INTERNAL://localhost:29092,EXTERNAL://192.168.43.195:9092
  #     KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
  #     KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
  #     KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
  #     KAFKA_CREATE_TOPICS: "news_feed:1:1"


  # USE THIS IF YOU WISH TO SETUP SPARK SERVER SEPARATELY. THIS CONSUMES QUITE A BIT OF MEMORY, HENCE COMMENTED OUT

  # spark:
  #   image: docker.io/bitnami/spark:latest
  #   ports:
  #     - "7077:7077"
  #     - "8082:8080"
  #   environment:
  #     - SPARK_MODE=master
  #     - SPARK_RPC_AUTHENTICATION_ENABLED=no
  #     - SPARK_RPC_ENCRYPTION_ENABLED=no
  #     - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
  #     - SPARK_SSL_ENABLED=no
  #   volumes:
  #     - /Users/admin/Documents/personal/mentor/IIIT-HyderabadCourse/Capstone-NewsClassifier:/opt/bitnami/spark/user_data
  # spark-worker-1:
  #   image: docker.io/bitnami/spark:latest
  #   environment:
  #     - SPARK_MODE=worker
  #     - SPARK_MASTER_URL=spark://spark:7077
  #     - SPARK_WORKER_MEMORY=1G
  #     - SPARK_WORKER_CORES=1
  #     - SPARK_RPC_AUTHENTICATION_ENABLED=no
  #     - SPARK_RPC_ENCRYPTION_ENABLED=no
  #     - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
  #     - SPARK_SSL_ENABLED=no
  #   volumes:
  #     - /Users/admin/Documents/personal/mentor/IIIT-HyderabadCourse/Capstone-NewsClassifier:/opt/bitnami/spark/user_data

  news-generator:
    image: usrivastava24/news-generator
    ports:
      - "5002:8000"
    env_file:
      - config/news_generator.env 
  news-producer:
    image: usrivastava24/news-producer
    depends_on:
      - news-generator
    ports:
      - "5003:8000"
    env_file:
      - config/news_producer.env 
  news-consumer:
    image: usrivastava24/news-consumer
    depends_on:
      - news-generator
      - news-producer
      - mongo
    ports:
      - "5004:8000"
    env_file:
      - config/news_consumer.env

  # THIS IS CONSUMING A LOT OF RESOURCES TO RUN MODEL TRAINING ON SPARK ON LOCAL MACHINE. RUN THIS AS STANDALONE SERVICE

  # model-trainer:
  #   image: usrivastava24/model-trainer
  #   depends_on:
  #     - mongo
  #   ports:
  #     - "5005:8000"
  #   env_file:
  #     - config/model_trainer.env
  model-prediction:
    image: usrivastava24/model-prediction
    ports:
      - "5006:8000"
    env_file:
      - config/model_prediction.env
    volumes:
      - /Users/admin/Documents/personal/mentor/IIIT-HyderabadCourse/Capstone-NewsClassifier/model-trainer/resources/saved-models:/resources/saved-models
      - /Users/admin/Documents/personal/mentor/IIIT-HyderabadCourse/Capstone-NewsClassifier/model-trainer/resources/saved-pipelines:/resources/saved-pipelines

